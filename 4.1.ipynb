{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c2239b",
   "metadata": {},
   "source": [
    "configuration of the small GPT-2 model (dummy class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1db544da",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # vocabulary of 50,257 words\n",
    "\"context_length\": 1024, #  maximum number of input tokens the model can handle\n",
    "\"emb_dim\": 768, # embedding size, transforming each token into a 768-dimensional vector\n",
    "\"n_heads\": 12, # count of attention heads in the multi-head attention mechanism\n",
    "\"n_layers\": 12, # number of transformer blocks in the model\n",
    "\"drop_rate\": 0.1, #  intensity of the dropout mechanism to prevent overfitting\n",
    "\"qkv_bias\": False # whether to include a bias vector in the Linear layers of the MHA for QKV computations\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e8e5a",
   "metadata": {},
   "source": [
    " placeholder GPT model architecture class\n",
    "\n",
    " -- An LLM consists of transformer blocks that contain masked MHAs. Weâ€™re transforming token IDs into words into vector representations then later on transferred into vocab size so that the LLM generates text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) # turns numbers into vectors\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) # adds info about position of each word\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential( # trf blocks is a series of dummy transformer blocks\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) #A\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]) #B\n",
    "        self.out_head = nn.Linear( # outhead is the final layer that takes the processed vectors and turns them back into a prediction for next word\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "        # DEF FORWARD; computes token and positional embeddings for the input indices, \n",
    "        # applies dropout, processes the data through\n",
    "        # the transformer blocks, applies normalization, and finally produces logits with the linear\n",
    "        # output layer.\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module): #C, tests if data flows correctly from embedding to layer\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x): #F, It doesnt do anything at first, just returns input\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module): #E, normalizing output of a given layer that go into the next layer for optimization\n",
    "    def __init__(self, normalized_shape, eps=1e-5): #F\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9fdf5",
   "metadata": {},
   "source": [
    "tokenize a batch consisting of two text\n",
    "inputs for the GPT model using the tiktoken tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6d1f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\" # Each word represents a token in output\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b944011",
   "metadata": {},
   "source": [
    "initialize a new 124 million parameter DummyGPTModel instance and feed it the\n",
    "tokenized batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88a2eb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M) # model initialized with random values via embedding layers\n",
    "logits = model(batch) #calling model\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits) #logits: last linear layer that returns its output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200479b9",
   "metadata": {},
   "source": [
    "implement a neural network layer with 5 inputs and 6 outputs that we apply to two input\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #will generate random sample\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU()) #ReLU is a non-linear activation function so network can learn more things\n",
    "out = layer(batch_example)\n",
    "print(out)\n",
    "\n",
    "# first row lists the layer outputs for the first input\n",
    "# and the second row lists the layer outputs for the second row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f7876",
   "metadata": {},
   "source": [
    "apply layer normalization to these outputs, let's examine the mean and\n",
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12f49898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True) #keepDim keeps dimensions and rows for efficiency\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "# first row in the mean tensor above contains the mean value for the first input row, and\n",
    "# the second output row contains the mean for the second input row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b13f51",
   "metadata": {},
   "source": [
    "apply layer normalization to the layer outputs we obtained earlier. The\n",
    "operation consists of subtracting the mean and dividing by the square root of the variance\n",
    "(also known as standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c40a426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47786e",
   "metadata": {},
   "source": [
    "turn off the scientific notation when printing tensor\n",
    "values by setting sci_mode to False for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "389127fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c09bf",
   "metadata": {},
   "source": [
    "Layer normalization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa2ec341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim): #initialization parameters\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5 # prevents division by 0, very small value placeholder\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # scale makes the 6 values trainable\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # shift, helps later when adding back value\n",
    "    def forward(self, x): \n",
    "        mean = x.mean(dim=-1, keepdim=True) # normalization, computed mean\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # norm, computer variance\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps) # norm, substracted mean divided by SD, EPS prevents div by 0 error\n",
    "        return self.scale * norm_x + self.shift # shift adds back the value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f05df7",
   "metadata": {},
   "source": [
    "below normalizes the values of each of the two inputs such that they have a mean of 0 and a\n",
    "variance of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da21a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example) # outputs transformed values\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True) # unbiased false is sample statistic\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
