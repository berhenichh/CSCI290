{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05efb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f551364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'My Lord Chancellor,\\n\\n'When I consider the Affair \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open( \"datashort_story2.txt\", \"r\" ) as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "raw_text[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5656cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8b3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52d5f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 3666, 4453, 19477, 11, 198, 198, 6, 2215, 314, 2074, 262, 6708, 958, 286, 281, 4479, 731, 86, 6346]\n"
     ]
    }
   ],
   "source": [
    "print(enc_text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2142d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'My\n"
     ]
    }
   ],
   "source": [
    "print( tokenizer.decode( enc_text[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214f906e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7407"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f27cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ' Target: My\n",
      "Input: 'My Target:  Lord\n",
      "Input: 'My Lord Target:  Chancellor\n",
      "Input: 'My Lord Chancellor Target: ,\n",
      "Input: 'My Lord Chancellor, Target: \n",
      "\n",
      "Input: 'My Lord Chancellor,\n",
      " Target: \n",
      "\n",
      "Input: 'My Lord Chancellor,\n",
      "\n",
      " Target: '\n",
      "Input: 'My Lord Chancellor,\n",
      "\n",
      "' Target: When\n",
      "Input: 'My Lord Chancellor,\n",
      "\n",
      "'When Target:  I\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print(\"Input:\", tokenizer.decode(enc_text[:i]), \"Target:\", tokenizer.decode([enc_text[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0caa8c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a70050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a085f249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[    6,  3666,  4453, 19477],\n",
      "        [   11,   198,   198,     6],\n",
      "        [ 2215,   314,  2074,   262],\n",
      "        [ 6708,   958,   286,   281],\n",
      "        [ 4479,   731,    86,  6346],\n",
      "        [  262,   734,  7973,    11],\n",
      "        [  355,   340,   318,  6241],\n",
      "        [  287,   262,  1811, 22698]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 3666,  4453, 19477,    11],\n",
      "        [  198,   198,     6,  2215],\n",
      "        [  314,  2074,   262,  6708],\n",
      "        [  958,   286,   281,  4479],\n",
      "        [  731,    86,  6346,   262],\n",
      "        [  734,  7973,    11,   355],\n",
      "        [  340,   318,  6241,   287],\n",
      "        [  262,  1811, 22698, 15370]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8c4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'My Lord Chancellor\n",
      ",\n",
      "\n",
      "'\n",
      "When I consider the\n",
      " Affair of an\n",
      " Union betwixt\n",
      " the two Nations,\n",
      " as it is expressed\n",
      " in the several Articles\n"
     ]
    }
   ],
   "source": [
    "# to apply the tokenizer's decoder to these IDs, the rows of the tensor `inputs` have to be converted into lists:\n",
    "for row in inputs:\n",
    "    print( tokenizer.decode( row.tolist() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9309d828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4449, -0.3830, -0.6007],\n",
      "        [ 0.0633,  0.3001,  0.8977],\n",
      "        [-0.3109,  1.0373, -0.2158],\n",
      "        [-0.2947, -0.1635, -0.4387],\n",
      "        [-0.0367, -1.2870, -0.8663],\n",
      "        [-0.3627, -0.9396,  0.0416]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we don't send these IDs to the LLM for training; we associate a vector a.k.a. tensor with each ID and then train the LLM on the vectors\n",
    "# as a first example, let's create embedding vectors of length 3 for each token in a vocabulary of 6 tokens\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "embedding = torch.nn.Embedding( vocab_size, output_dim )\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e94ebbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4449, -0.3830, -0.6007],\n",
       "        [ 0.0633,  0.3001,  0.8977],\n",
       "        [-0.3109,  1.0373, -0.2158],\n",
       "        [-0.2947, -0.1635, -0.4387],\n",
       "        [-0.0367, -1.2870, -0.8663],\n",
       "        [-0.3627, -0.9396,  0.0416]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you just want the tensor part of this without the requires_grad=True bit\n",
    "# method 1:\n",
    "embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635ced63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4449, -0.3830, -0.6007],\n",
       "        [ 0.0633,  0.3001,  0.8977],\n",
       "        [-0.3109,  1.0373, -0.2158],\n",
       "        [-0.2947, -0.1635, -0.4387],\n",
       "        [-0.0367, -1.2870, -0.8663],\n",
       "        [-0.3627, -0.9396,  0.0416]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you just want the tensor part of this without the requires_grad=True bit\n",
    "# method 1:\n",
    "embedding.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4a6f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call this A for some examples:\n",
    "A = embedding.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21ca34aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4449, -0.3830, -0.6007])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first row:\n",
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06fc0140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0633, 0.3001, 0.8977])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second row:\n",
    "A[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78f7546d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4449,  0.0633, -0.3109, -0.2947, -0.0367, -0.3627])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first column:\n",
    "A[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9bf6429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8977)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element in row 2, column 3:\n",
    "A[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3be2fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2000, 2.1000])\n",
      "tensor([2.7000, 1.5000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# to create a tensor directly:\n",
    "x = torch.tensor([1.2,2.1])\n",
    "y = torch.tensor([2.7,1.5])\n",
    "print(x) \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cb38cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3900)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.dot( x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4ba853c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.390000000000001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check:\n",
    "1.2*2.7 + 2.1*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d4c2520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.0210,  0.7720,  0.3229,  0.5406,  1.8349, -0.1785, -0.4625, -0.3050],\n",
      "        [-0.6549, -0.0324, -0.6208,  0.3278,  0.6982, -0.6581,  0.0054, -1.3516],\n",
      "        [-1.1034,  0.4322,  1.5395, -1.0670, -0.1930,  0.4849,  0.9935, -0.1667],\n",
      "        [ 0.5642,  1.3899, -0.2606, -1.9455,  1.2959,  1.2877,  1.3191, -1.1651]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 4\n",
    "output_dim = 8\n",
    "inputs = torch.nn.Embedding( vocab_size, output_dim )\n",
    "print( inputs.weight )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb98c36",
   "metadata": {},
   "source": [
    "Embedding transforms each token ID into a continuous vector representation so\n",
    "in this example, with a vocabulary size of 8 and an output dimension of 4, each of the 8 tokens is represented as a 4-dimensional vector.\n",
    "This is necessary because neural networks cant learn meaningful patterns from integers alone. Embeddings allow the model to work in a continuous space where distances and directions represent relationships which enables it to generalize and learn contextual meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0011b400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0210,  0.7720,  0.3229,  0.5406,  1.8349, -0.1785, -0.4625, -0.3050],\n",
       "        [-0.6549, -0.0324, -0.6208,  0.3278,  0.6982, -0.6581,  0.0054, -1.3516],\n",
       "        [-1.1034,  0.4322,  1.5395, -1.0670, -0.1930,  0.4849,  0.9935, -0.1667],\n",
       "        [ 0.5642,  1.3899, -0.2606, -1.9455,  1.2959,  1.2877,  1.3191, -1.1651]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.weight.data\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c545cfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "680a14a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0210223197937012, 0.7719696760177612, 0.32292377948760986, 0.5405638813972473, 1.8348582983016968, -0.1784828007221222, -0.4625184237957001, -0.30498939752578735]\n",
      "[-0.6548746228218079, -0.03241903707385063, -0.6208429932594299, 0.3278284966945648, 0.6982240080833435, -0.6581456065177917, 0.005382075440138578, -1.3515597581863403]\n",
      "[-1.1034115552902222, 0.43220487236976624, 1.5394843816757202, -1.0669578313827515, -0.19296561181545258, 0.4849236309528351, 0.9934572577476501, -0.16671492159366608]\n",
      "[0.5642220377922058, 1.3898890018463135, -0.2606104016304016, -1.9454749822616577, 1.2959389686584473, 1.2876970767974854, 1.31905198097229, -1.1650508642196655]\n"
     ]
    }
   ],
   "source": [
    "for row in inputs:\n",
    "    print(row.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4c68644",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([1.1, 2.3])\n",
    "y = torch.Tensor([3.4,-2.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8566e390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0899999999999999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "1.1*3.4 + 2.3*(-2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d98976ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0900)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot( x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c7a971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1034,  0.4322,  1.5395, -1.0670, -0.1930,  0.4849,  0.9935, -0.1667])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[2]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a7d6cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.7218)\n",
      "tensor(-0.8202)\n",
      "tensor(6.1999)\n",
      "tensor(3.5317)\n"
     ]
    }
   ],
   "source": [
    "for i in range( len( inputs )):\n",
    "    print( torch.dot( query, inputs[i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "793dc270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7218, -0.8202,  6.1999,  3.5317])\n"
     ]
    }
   ],
   "source": [
    "attention_scores_2 = torch.zeros( len(inputs) )\n",
    "for i in range( len( inputs )):\n",
    "    attention_scores_2[i] = torch.dot( query, inputs[i] )\n",
    "print( attention_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f50691e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3885e-04, 8.3481e-04, 9.3402e-01, 6.4802e-02])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2 = torch.softmax( attention_scores_2, dim = 0 )\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "221b1410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attention_weights_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "274d8c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9943,  0.4940,  1.4206, -1.1222, -0.0951,  0.5358,  1.0132, -0.2324])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = torch.zeros( query.shape )\n",
    "for i in range( len( attention_weights_2 ) ):\n",
    "    context_vector_2 += attention_weights_2[i]*inputs[i]\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "287d3d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0210,  0.7720,  0.3229,  0.5406,  1.8349, -0.1785, -0.4625, -0.3050],\n",
       "        [-0.6549, -0.0324, -0.6208,  0.3278,  0.6982, -0.6581,  0.0054, -1.3516],\n",
       "        [-1.1034,  0.4322,  1.5395, -1.0670, -0.1930,  0.4849,  0.9935, -0.1667],\n",
       "        [ 0.5642,  1.3899, -0.2606, -1.9455,  1.2959,  1.2877,  1.3191, -1.1651]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "315228e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0210, -0.6549, -1.1034,  0.5642],\n",
       "        [ 0.7720, -0.0324,  0.4322,  1.3899],\n",
       "        [ 0.3229, -0.6208,  1.5395, -0.2606],\n",
       "        [ 0.5406,  0.3278, -1.0670, -1.9455],\n",
       "        [ 1.8349,  0.6982, -0.1930,  1.2959],\n",
       "        [-0.1785, -0.6581,  0.4849,  1.2877],\n",
       "        [-0.4625,  0.0054,  0.9935,  1.3191],\n",
       "        [-0.3050, -1.3516, -0.1667, -1.1651]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f770226b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7404,  1.0914, -1.7218,  2.4065],\n",
       "        [ 1.0914,  3.6702, -0.8202,  0.7486],\n",
       "        [-1.7218, -0.8202,  6.1999,  3.5317],\n",
       "        [ 2.4065,  0.7486,  3.5317, 12.5378]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all of the attention scores via a matrix multiplication:\n",
    "attention_scores = inputs @ inputs.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4ea274f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.5621e-01, 9.1518e-03, 5.4920e-04, 3.4092e-02],\n",
       "        [6.6491e-02, 8.7649e-01, 9.8305e-03, 4.7193e-02],\n",
       "        [3.3885e-04, 8.3481e-04, 9.3402e-01, 6.4802e-02],\n",
       "        [3.9807e-05, 7.5845e-06, 1.2264e-04, 9.9983e-01]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = torch.softmax( attention_scores, dim = -1 )\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9995446a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8c09d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9889,  0.7855,  0.2951,  0.4530,  1.8050, -0.1325, -0.3967, -0.3438],\n",
       "        [-0.4903,  0.0928, -0.5199,  0.2210,  0.7932, -0.5232,  0.0460, -1.2615],\n",
       "        [-0.9943,  0.4940,  1.4206, -1.1222, -0.0951,  0.5358,  1.0132, -0.2324],\n",
       "        [ 0.5640,  1.3897, -0.2604, -1.9453,  1.2958,  1.2875,  1.3189, -1.1649]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = attention_weights @ inputs\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411debf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
