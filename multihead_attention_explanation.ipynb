{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Fsye9eKX8iRI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will define the class inheriting from **nn.Module** then assert **d_out % num_heads == 0** to ensure even splitting of dimensions for heads"
      ],
      "metadata": {
        "id": "cjuLeORv8jCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        ## then initialize linear layers for Q, K, V projections from d_in to d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        ## then add linear to combine heads post-attention. Dropout for regularization\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        ## then register a upper-triangular mask as buffer so we have a causal mask to block future tokens\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n",
        "        # this will result in errors in the mask creation further below.\n",
        "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n",
        "        # do not exceed `context_length` before reaching this forward method.\n",
        "\n",
        "        ## then we'll set a seed for reproductibility and create a random input to instantiate class with parameters\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # Sample parameters and data\n",
        "        b = 2\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # Sample parameters and data\n",
        "        b = 2\n",
        "        num_tokens = 3\n",
        "        d_in = 4\n",
        "        d_out = 6\n",
        "        num_heads = 3\n",
        "        dropout = 0.0\n",
        "        qkv_bias = False\n",
        "\n",
        "        x = torch.randn(b, num_tokens, d_in)\n",
        "        print(\"Input x shape:\", x.shape)\n",
        "        print(\"Input x:\\n\", x)\n",
        "\n",
        "        mha = MultiHeadAttention(d_in, d_out, num_tokens, dropout, num_heads, qkv_bias)\n",
        "\n",
        "        ## transform input to attention spaces and d_out concatenated for all heads.\n",
        "        keys = mha.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        print(\"Keys shape:\", keys.shape)\n",
        "        print(\"Keys:\\n\", keys)\n",
        "\n",
        "        queries = mha.W_query(x)\n",
        "        print(\"\\nQueries shape:\", queries.shape)\n",
        "        print(\"Queries:\\n\", queries)\n",
        "\n",
        "        values = mha.W_value(x)\n",
        "        print(\"\\nValues shape:\", values.shape)\n",
        "        print(\"Values:\\n\", values)\n",
        "\n",
        "        ## then reshape last dim to add num_heads and head_dim\n",
        "        keys = keys.view(b, num_tokens, mha.num_heads, mha.head_dim)\n",
        "        print(\"Keys after view shape:\", keys.shape)\n",
        "        print(\"Keys after view:\\n\", keys)\n",
        "\n",
        "        values = values.view(b, num_tokens, mha.num_heads, mha.head_dim)\n",
        "        print(\"\\nValues after view shape:\", values.shape)\n",
        "        print(\"Values after view:\\n\", values)\n",
        "\n",
        "        queries = queries.view(b, num_tokens, mha.num_heads, mha.head_dim)\n",
        "        print(\"\\nQueries after view shape:\", queries.shape)\n",
        "        print(\"Queries after view:\\n\", queries)\n",
        "\n",
        "        ## then swap num_tokens and num_heads because it makes shape (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        print(\"Keys after transpose shape:\", keys.shape)\n",
        "        print(\"Keys after transpose:\\n\", keys)\n",
        "\n",
        "        queries = queries.transpose(1, 2)\n",
        "        print(\"\\nQueries after transpose shape:\", queries.shape)\n",
        "        print(\"Queries after transpose:\\n\", queries)\n",
        "\n",
        "        values = values.transpose(1, 2)\n",
        "        print(\"\\nValues after transpose shape:\", values.shape)\n",
        "        print(\"Values after transpose:\\n\", values)\n",
        "\n",
        "        ## Batched dot-product\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "        print(\"Attention scores shape:\", attn_scores.shape)\n",
        "        print(\"Attention scores:\\n\", attn_scores)\n",
        "\n",
        "        ## Truncates mask to num_tokens, convert to bool, fill attn_scores with -inf where masked\n",
        "        mask_bool = mha.mask.bool()[:num_tokens, :num_tokens]\n",
        "        print(\"Mask shape:\", mask_bool.shape)\n",
        "        print(\"Mask:\\n\", mask_bool)\n",
        "\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "        print(\"\\nMasked attention scores shape:\", attn_scores.shape)\n",
        "        print(\"Masked attention scores:\\n\", attn_scores)\n",
        "\n",
        "        ## below: scaling stabilizes, softmax to probabilities, dropout regularizes and batches are efficient\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        print(\"Attention weights after softmax shape:\", attn_weights.shape)\n",
        "        print(\"Attention weights after softmax:\\n\", attn_weights)\n",
        "\n",
        "        attn_weights = mha.dropout(attn_weights)\n",
        "        print(\"\\nAfter dropout shape:\", attn_weights.shape)\n",
        "        print(\"After dropout:\\n\", attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        print(\"Context vec after matrix multiplication and transpose shape:\", context_vec.shape)\n",
        "        print(\"Context vec after matrix multiplication and transpose:\\n\", context_vec)\n",
        "\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, mha.d_out)\n",
        "        print(\"Context vec after view shape:\", context_vec.shape)\n",
        "        print(\"Context vec after view:\\n\", context_vec)\n",
        "\n",
        "        context_vec = mha.out_proj(context_vec) # optional projection\n",
        "        print(\"\\nFinal output shape:\", context_vec.shape)\n",
        "        print(\"Final output:\\n\", context_vec)\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "# test\n",
        "x = torch.randn(2, 3, 4)\n",
        "mha = MultiHeadAttention(4, 6, 3, 0.0, 3, False)\n",
        "output = mha.forward(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnnKq9nCDvlf",
        "outputId": "64019972-71d6-4a71-9ce9-4d41f806454d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input x shape: torch.Size([2, 3, 4])\n",
            "Input x:\n",
            " tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
            "         [ 0.6784, -1.2345, -0.0431, -1.6047],\n",
            "         [ 0.3559, -0.6866, -0.4934,  0.2415]],\n",
            "\n",
            "        [[-1.1109,  0.0915, -2.3169, -0.2168],\n",
            "         [-0.3097, -0.3957,  0.8034, -0.6216],\n",
            "         [-0.5920, -0.0631, -0.8286,  0.3309]]])\n",
            "Keys shape: torch.Size([2, 3, 6])\n",
            "Keys:\n",
            " tensor([[[ 1.1378, -0.9639,  0.6903, -1.0604, -0.3400,  1.4839],\n",
            "         [ 0.6902, -0.1209,  1.4849, -1.1526,  0.5585,  0.9634],\n",
            "         [-0.1501, -0.0830,  0.6047, -0.2467, -0.0375, -0.0455]],\n",
            "\n",
            "        [[-1.0763, -0.7741,  0.6616, -0.1127, -0.3152, -0.9264],\n",
            "         [ 0.5111,  0.4681, -0.1410, -0.0904,  0.5536,  0.4242],\n",
            "         [-0.5258, -0.1221,  0.0227,  0.1755, -0.1123, -0.5377]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "Queries shape: torch.Size([2, 3, 6])\n",
            "Queries:\n",
            " tensor([[[-0.6932,  0.3034, -0.0030, -0.4352,  1.3922,  0.0545],\n",
            "         [-0.4669,  0.3099,  0.8798, -1.1052,  0.6712,  0.7123],\n",
            "         [ 0.1449,  0.1321,  0.4971, -0.4392, -0.2166,  0.1279]],\n",
            "\n",
            "        [[-0.7460, -1.0292,  0.8787,  0.0850, -1.0061, -0.3861],\n",
            "         [-0.0964,  0.1733, -0.2356, -0.0566,  0.4897,  0.3562],\n",
            "         [-0.0976, -0.3669,  0.2151,  0.1504, -0.5348, -0.1818]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "Values shape: torch.Size([2, 3, 6])\n",
            "Values:\n",
            " tensor([[[ 0.1041,  1.3153,  1.4903,  0.3388, -0.2877, -1.0654],\n",
            "         [-0.1594,  0.2265,  1.3029,  0.2368,  0.8897, -0.0769],\n",
            "         [-0.2774, -0.0129,  0.2494, -0.2427,  0.3815, -0.1609]],\n",
            "\n",
            "        [[ 0.0770,  0.5980, -0.0085,  0.5611,  0.9982, -0.7215],\n",
            "         [ 0.1820, -0.3522,  0.0833,  0.2118, -0.0381,  0.6044],\n",
            "         [ 0.0100,  0.0142, -0.2736,  0.0962,  0.2750, -0.1037]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Keys after view shape: torch.Size([2, 3, 3, 2])\n",
            "Keys after view:\n",
            " tensor([[[[ 1.1378, -0.9639],\n",
            "          [ 0.6903, -1.0604],\n",
            "          [-0.3400,  1.4839]],\n",
            "\n",
            "         [[ 0.6902, -0.1209],\n",
            "          [ 1.4849, -1.1526],\n",
            "          [ 0.5585,  0.9634]],\n",
            "\n",
            "         [[-0.1501, -0.0830],\n",
            "          [ 0.6047, -0.2467],\n",
            "          [-0.0375, -0.0455]]],\n",
            "\n",
            "\n",
            "        [[[-1.0763, -0.7741],\n",
            "          [ 0.6616, -0.1127],\n",
            "          [-0.3152, -0.9264]],\n",
            "\n",
            "         [[ 0.5111,  0.4681],\n",
            "          [-0.1410, -0.0904],\n",
            "          [ 0.5536,  0.4242]],\n",
            "\n",
            "         [[-0.5258, -0.1221],\n",
            "          [ 0.0227,  0.1755],\n",
            "          [-0.1123, -0.5377]]]], grad_fn=<ViewBackward0>)\n",
            "\n",
            "Values after view shape: torch.Size([2, 3, 3, 2])\n",
            "Values after view:\n",
            " tensor([[[[ 0.1041,  1.3153],\n",
            "          [ 1.4903,  0.3388],\n",
            "          [-0.2877, -1.0654]],\n",
            "\n",
            "         [[-0.1594,  0.2265],\n",
            "          [ 1.3029,  0.2368],\n",
            "          [ 0.8897, -0.0769]],\n",
            "\n",
            "         [[-0.2774, -0.0129],\n",
            "          [ 0.2494, -0.2427],\n",
            "          [ 0.3815, -0.1609]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0770,  0.5980],\n",
            "          [-0.0085,  0.5611],\n",
            "          [ 0.9982, -0.7215]],\n",
            "\n",
            "         [[ 0.1820, -0.3522],\n",
            "          [ 0.0833,  0.2118],\n",
            "          [-0.0381,  0.6044]],\n",
            "\n",
            "         [[ 0.0100,  0.0142],\n",
            "          [-0.2736,  0.0962],\n",
            "          [ 0.2750, -0.1037]]]], grad_fn=<ViewBackward0>)\n",
            "\n",
            "Queries after view shape: torch.Size([2, 3, 3, 2])\n",
            "Queries after view:\n",
            " tensor([[[[-0.6932,  0.3034],\n",
            "          [-0.0030, -0.4352],\n",
            "          [ 1.3922,  0.0545]],\n",
            "\n",
            "         [[-0.4669,  0.3099],\n",
            "          [ 0.8798, -1.1052],\n",
            "          [ 0.6712,  0.7123]],\n",
            "\n",
            "         [[ 0.1449,  0.1321],\n",
            "          [ 0.4971, -0.4392],\n",
            "          [-0.2166,  0.1279]]],\n",
            "\n",
            "\n",
            "        [[[-0.7460, -1.0292],\n",
            "          [ 0.8787,  0.0850],\n",
            "          [-1.0061, -0.3861]],\n",
            "\n",
            "         [[-0.0964,  0.1733],\n",
            "          [-0.2356, -0.0566],\n",
            "          [ 0.4897,  0.3562]],\n",
            "\n",
            "         [[-0.0976, -0.3669],\n",
            "          [ 0.2151,  0.1504],\n",
            "          [-0.5348, -0.1818]]]], grad_fn=<ViewBackward0>)\n",
            "Keys after transpose shape: torch.Size([2, 3, 3, 2])\n",
            "Keys after transpose:\n",
            " tensor([[[[ 1.1378, -0.9639],\n",
            "          [ 0.6902, -0.1209],\n",
            "          [-0.1501, -0.0830]],\n",
            "\n",
            "         [[ 0.6903, -1.0604],\n",
            "          [ 1.4849, -1.1526],\n",
            "          [ 0.6047, -0.2467]],\n",
            "\n",
            "         [[-0.3400,  1.4839],\n",
            "          [ 0.5585,  0.9634],\n",
            "          [-0.0375, -0.0455]]],\n",
            "\n",
            "\n",
            "        [[[-1.0763, -0.7741],\n",
            "          [ 0.5111,  0.4681],\n",
            "          [-0.5258, -0.1221]],\n",
            "\n",
            "         [[ 0.6616, -0.1127],\n",
            "          [-0.1410, -0.0904],\n",
            "          [ 0.0227,  0.1755]],\n",
            "\n",
            "         [[-0.3152, -0.9264],\n",
            "          [ 0.5536,  0.4242],\n",
            "          [-0.1123, -0.5377]]]], grad_fn=<TransposeBackward0>)\n",
            "\n",
            "Queries after transpose shape: torch.Size([2, 3, 3, 2])\n",
            "Queries after transpose:\n",
            " tensor([[[[-0.6932,  0.3034],\n",
            "          [-0.4669,  0.3099],\n",
            "          [ 0.1449,  0.1321]],\n",
            "\n",
            "         [[-0.0030, -0.4352],\n",
            "          [ 0.8798, -1.1052],\n",
            "          [ 0.4971, -0.4392]],\n",
            "\n",
            "         [[ 1.3922,  0.0545],\n",
            "          [ 0.6712,  0.7123],\n",
            "          [-0.2166,  0.1279]]],\n",
            "\n",
            "\n",
            "        [[[-0.7460, -1.0292],\n",
            "          [-0.0964,  0.1733],\n",
            "          [-0.0976, -0.3669]],\n",
            "\n",
            "         [[ 0.8787,  0.0850],\n",
            "          [-0.2356, -0.0566],\n",
            "          [ 0.2151,  0.1504]],\n",
            "\n",
            "         [[-1.0061, -0.3861],\n",
            "          [ 0.4897,  0.3562],\n",
            "          [-0.5348, -0.1818]]]], grad_fn=<TransposeBackward0>)\n",
            "\n",
            "Values after transpose shape: torch.Size([2, 3, 3, 2])\n",
            "Values after transpose:\n",
            " tensor([[[[ 0.1041,  1.3153],\n",
            "          [-0.1594,  0.2265],\n",
            "          [-0.2774, -0.0129]],\n",
            "\n",
            "         [[ 1.4903,  0.3388],\n",
            "          [ 1.3029,  0.2368],\n",
            "          [ 0.2494, -0.2427]],\n",
            "\n",
            "         [[-0.2877, -1.0654],\n",
            "          [ 0.8897, -0.0769],\n",
            "          [ 0.3815, -0.1609]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0770,  0.5980],\n",
            "          [ 0.1820, -0.3522],\n",
            "          [ 0.0100,  0.0142]],\n",
            "\n",
            "         [[-0.0085,  0.5611],\n",
            "          [ 0.0833,  0.2118],\n",
            "          [-0.2736,  0.0962]],\n",
            "\n",
            "         [[ 0.9982, -0.7215],\n",
            "          [-0.0381,  0.6044],\n",
            "          [ 0.2750, -0.1037]]]], grad_fn=<TransposeBackward0>)\n",
            "Attention scores shape: torch.Size([2, 3, 3, 3])\n",
            "Attention scores:\n",
            " tensor([[[[-1.0812e+00, -5.1514e-01,  7.8872e-02],\n",
            "          [-8.2996e-01, -3.5972e-01,  4.4360e-02],\n",
            "          [ 3.7481e-02,  8.4034e-02, -3.2717e-02]],\n",
            "\n",
            "         [[ 4.5934e-01,  4.9704e-01,  1.0553e-01],\n",
            "          [ 1.7793e+00,  2.5803e+00,  8.0477e-01],\n",
            "          [ 8.0893e-01,  1.2444e+00,  4.0902e-01]],\n",
            "\n",
            "         [[-3.9249e-01,  8.3009e-01, -5.4736e-02],\n",
            "          [ 8.2878e-01,  1.0612e+00, -5.7583e-02],\n",
            "          [ 2.6341e-01,  2.2322e-03,  2.3156e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5997e+00, -8.6310e-01,  5.1794e-01],\n",
            "          [-3.0386e-02,  3.1837e-02,  2.9515e-02],\n",
            "          [ 3.8906e-01, -2.2163e-01,  9.6132e-02]],\n",
            "\n",
            "         [[ 5.7175e-01, -1.3159e-01,  3.4840e-02],\n",
            "          [-1.4951e-01,  3.8341e-02, -1.5273e-02],\n",
            "          [ 1.2535e-01, -4.3919e-02,  3.1262e-02]],\n",
            "\n",
            "         [[ 6.7480e-01, -7.2077e-01,  3.2061e-01],\n",
            "          [-4.8433e-01,  4.2220e-01, -2.4652e-01],\n",
            "          [ 3.3703e-01, -3.7323e-01,  1.5784e-01]]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "Mask shape: torch.Size([3, 3])\n",
            "Mask:\n",
            " tensor([[False,  True,  True],\n",
            "        [False, False,  True],\n",
            "        [False, False, False]])\n",
            "\n",
            "Masked attention scores shape: torch.Size([2, 3, 3, 3])\n",
            "Masked attention scores:\n",
            " tensor([[[[-1.0812e+00,        -inf,        -inf],\n",
            "          [-8.2996e-01, -3.5972e-01,        -inf],\n",
            "          [ 3.7481e-02,  8.4034e-02, -3.2717e-02]],\n",
            "\n",
            "         [[ 4.5934e-01,        -inf,        -inf],\n",
            "          [ 1.7793e+00,  2.5803e+00,        -inf],\n",
            "          [ 8.0893e-01,  1.2444e+00,  4.0902e-01]],\n",
            "\n",
            "         [[-3.9249e-01,        -inf,        -inf],\n",
            "          [ 8.2878e-01,  1.0612e+00,        -inf],\n",
            "          [ 2.6341e-01,  2.2322e-03,  2.3156e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5997e+00,        -inf,        -inf],\n",
            "          [-3.0386e-02,  3.1837e-02,        -inf],\n",
            "          [ 3.8906e-01, -2.2163e-01,  9.6132e-02]],\n",
            "\n",
            "         [[ 5.7175e-01,        -inf,        -inf],\n",
            "          [-1.4951e-01,  3.8341e-02,        -inf],\n",
            "          [ 1.2535e-01, -4.3919e-02,  3.1262e-02]],\n",
            "\n",
            "         [[ 6.7480e-01,        -inf,        -inf],\n",
            "          [-4.8433e-01,  4.2220e-01,        -inf],\n",
            "          [ 3.3703e-01, -3.7323e-01,  1.5784e-01]]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "Attention weights after softmax shape: torch.Size([2, 3, 3, 3])\n",
            "Attention weights after softmax:\n",
            " tensor([[[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4176, 0.5824, 0.0000],\n",
            "          [0.3350, 0.3462, 0.3188]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.3621, 0.6379, 0.0000],\n",
            "          [0.3211, 0.4369, 0.2420]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4590, 0.5410, 0.0000],\n",
            "          [0.3755, 0.3122, 0.3122]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4890, 0.5110, 0.0000],\n",
            "          [0.4061, 0.2637, 0.3302]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4668, 0.5332, 0.0000],\n",
            "          [0.3543, 0.3143, 0.3315]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.3450, 0.6550, 0.0000],\n",
            "          [0.4022, 0.2434, 0.3544]]]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "After dropout shape: torch.Size([2, 3, 3, 3])\n",
            "After dropout:\n",
            " tensor([[[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4176, 0.5824, 0.0000],\n",
            "          [0.3350, 0.3462, 0.3188]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.3621, 0.6379, 0.0000],\n",
            "          [0.3211, 0.4369, 0.2420]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4590, 0.5410, 0.0000],\n",
            "          [0.3755, 0.3122, 0.3122]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000],\n",
            "          [0.4890, 0.5110, 0.0000],\n",
            "          [0.4061, 0.2637, 0.3302]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.4668, 0.5332, 0.0000],\n",
            "          [0.3543, 0.3143, 0.3315]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000],\n",
            "          [0.3450, 0.6550, 0.0000],\n",
            "          [0.4022, 0.2434, 0.3544]]]], grad_fn=<SoftmaxBackward0>)\n",
            "Context vec after matmul and transpose shape: torch.Size([2, 3, 3, 2])\n",
            "Context vec after matrix multiplication and transpose:\n",
            " tensor([[[[ 0.1041,  1.3153],\n",
            "          [ 1.4903,  0.3388],\n",
            "          [-0.2877, -1.0654]],\n",
            "\n",
            "         [[-0.0493,  0.6812],\n",
            "          [ 1.3707,  0.2737],\n",
            "          [ 0.3492, -0.5306]],\n",
            "\n",
            "         [[-0.1087,  0.5149],\n",
            "          [ 1.1081,  0.1535],\n",
            "          [ 0.2888, -0.4744]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0770,  0.5980],\n",
            "          [-0.0085,  0.5611],\n",
            "          [ 0.9982, -0.7215]],\n",
            "\n",
            "         [[ 0.1306,  0.1124],\n",
            "          [ 0.0404,  0.3749],\n",
            "          [ 0.3194,  0.1469]],\n",
            "\n",
            "         [[ 0.0826,  0.1547],\n",
            "          [-0.0675,  0.2973],\n",
            "          [ 0.4897, -0.1798]]]], grad_fn=<TransposeBackward0>)\n",
            "Context vec after view shape: torch.Size([2, 3, 6])\n",
            "Context vec after view:\n",
            " tensor([[[ 0.1041,  1.3153,  1.4903,  0.3388, -0.2877, -1.0654],\n",
            "         [-0.0493,  0.6812,  1.3707,  0.2737,  0.3492, -0.5306],\n",
            "         [-0.1087,  0.5149,  1.1081,  0.1535,  0.2888, -0.4744]],\n",
            "\n",
            "        [[ 0.0770,  0.5980, -0.0085,  0.5611,  0.9982, -0.7215],\n",
            "         [ 0.1306,  0.1124,  0.0404,  0.3749,  0.3194,  0.1469],\n",
            "         [ 0.0826,  0.1547, -0.0675,  0.2973,  0.4897, -0.1798]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "Final output shape: torch.Size([2, 3, 6])\n",
            "Final output:\n",
            " tensor([[[-0.1971, -0.4104, -0.5693, -0.1921, -0.1588, -1.1561],\n",
            "         [-0.0855, -0.3846, -0.1629, -0.4590,  0.0645, -0.6992],\n",
            "         [ 0.0330, -0.3294, -0.0564, -0.4462,  0.1037, -0.6533]],\n",
            "\n",
            "        [[ 0.4028, -0.3973,  0.1593,  0.0897,  0.0358, -0.4404],\n",
            "         [ 0.2513, -0.4808,  0.0103, -0.0823, -0.1618, -0.1548],\n",
            "         [ 0.3970, -0.3854,  0.1386, -0.0465, -0.0318, -0.2529]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}